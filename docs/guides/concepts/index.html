<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper docs-doc-page docs-version-current plugin-docs plugin-id-default docs-doc-id-guides/concepts">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v2.2.0">
<title data-rh="true">Training SDXL LoRa&#x27;s | Eden</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:url" content="https://docs.eden.art/docs/guides/concepts"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="Training SDXL LoRa&#x27;s | Eden"><meta data-rh="true" name="description" content="A limitation of generative models (including Eden&#x27;s base model) is that they can only generate things they&#x27;ve been trained on. But what if you want to consistently compose with a specific object, person&#x27;s face, or artistic style which is not found in the original training data? This is where Concepts come in."><meta data-rh="true" property="og:description" content="A limitation of generative models (including Eden&#x27;s base model) is that they can only generate things they&#x27;ve been trained on. But what if you want to consistently compose with a specific object, person&#x27;s face, or artistic style which is not found in the original training data? This is where Concepts come in."><link data-rh="true" rel="icon" href="/img/favicon.ico"><link data-rh="true" rel="canonical" href="https://docs.eden.art/docs/guides/concepts"><link data-rh="true" rel="alternate" href="https://docs.eden.art/docs/guides/concepts" hreflang="en"><link data-rh="true" rel="alternate" href="https://docs.eden.art/docs/guides/concepts" hreflang="x-default"><link rel="stylesheet" href="/assets/css/styles.80576e75.css">
<link rel="preload" href="/assets/js/runtime~main.3ed09d88.js" as="script">
<link rel="preload" href="/assets/js/main.e16c5f17.js" as="script">
</head>
<body class="navigation-with-keyboard">
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){var t=null;try{t=localStorage.getItem("theme")}catch(t){}return t}();t(null!==e?e:"light")}()</script><div id="__docusaurus">
<div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#docusaurus_skipToContent_fallback">Skip to main content</a></div><nav class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/"><div class="navbar__logo"><img src="/img/eden.png" alt="Eden" class="themedImage_ToTc themedImage--light_HNdA"><img src="/img/eden.png" alt="Eden" class="themedImage_ToTc themedImage--dark_i4oU"></div><b class="navbar__title text--truncate">Eden</b></a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/docs/category/overview">Docs</a></div><div class="navbar__items navbar__items--right"><a href="https://app.eden.art" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">App<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="Switch between dark and light mode (currently light mode)" aria-label="Switch between dark and light mode (currently light mode)" aria-live="polite"><svg viewBox="0 0 24 24" width="24" height="24" class="lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" class="darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg></button></div><div class="searchBox_ZlJk"></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="docusaurus_skipToContent_fallback" class="main-wrapper mainWrapper_z2l0 docsWrapper_BCFX"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docPage__5DB"><aside class="theme-doc-sidebar-container docSidebarContainer_b6E3"><div class="sidebar_njMd"><nav class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link" href="/docs/category/overview">Overview</a></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/overview/intro">About Eden</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/overview/manna">Manna</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/overview/tos">Terms of Service</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/overview/privacy">Privacy policy</a></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--active" href="/docs/category/guides">Guides</a></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/docs/guides/concepts">Training SDXL LoRa&#x27;s</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/guides/agents">Creative AI Agents (w FLUX LoRA)</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/guides/sdk">JavaScript SDK</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/guides/python-sdk">Python SDK</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/guides/generators">Custom hosted endpoints</a></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link" href="/docs/category/tools">Tools</a></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/tools/animate_3D">Animate an image with depth</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/tools/background_removal">Remove Background</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/tools/face_styler">Create a stylized selfie</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/tools/flux_dev">Create an image (Advanced)</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/tools/flux_inpainting">Replace part of an image (Inpaint)</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/tools/frame_interpolation">Keyframe Interpolation</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/tools/layer_diffusion">Create an image with transparency</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/tools/mars-id">ID Generator</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/tools/ominicontrol">Instant LoRA</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/tools/outpaint">Extend an image (Outpaint)</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/tools/remix_flux_schnell">Remix an Image</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/tools/stable_audio">Create Sound Effects</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/tools/texture_flow">TextureFlow</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/tools/txt2img">Create an image (SDXL)</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/tools/txt2vid">Create a video from text</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/tools/upscaler">Upscale an image</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/tools/vid2vid_sdxl">Stylize a video</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/tools/video_FX">AI Video Effects</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/tools/video_upscaler">Upscale a video</a></li></ul></li></ul></nav></div></aside><main class="docMainContainer_gTbr"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Alpn" aria-label="Breadcrumbs"><ul class="breadcrumbs" itemscope="" itemtype="https://schema.org/BreadcrumbList"><li class="breadcrumbs__item"><a aria-label="Home page" class="breadcrumbs__link" href="/docs/category/overview"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_xK9p"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li itemscope="" itemprop="itemListElement" itemtype="https://schema.org/ListItem" class="breadcrumbs__item"><a class="breadcrumbs__link" itemprop="item" href="/docs/category/guides"><span itemprop="name">Guides</span></a><meta itemprop="position" content="1"></li><li itemscope="" itemprop="itemListElement" itemtype="https://schema.org/ListItem" class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link" itemprop="name">Training SDXL LoRa&#x27;s</span><meta itemprop="position" content="2"></li></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><h1>Training SDXL LoRa&#x27;s</h1><p>A limitation of generative models (including Eden&#x27;s base model) is that they can only generate things they&#x27;ve been trained on. But what if you want to consistently compose with a specific object, person&#x27;s face, or artistic style which is not found in the original training data? This is where <em>Concepts</em> come in.</p><p>Concepts are custom characters, objects, styles, or specific people which have been trained and added by Eden users to the Eden generators&#x27; knowledge base, using the <a href="https://arxiv.org/abs/2106.09685" target="_blank" rel="noopener noreferrer">LoRA technique</a>. With concepts, users are able to consistently reproduce specific content and styles in their creations.</p><p>Concepts are first trained by uploading example images to the <a href="https://beta.eden.art/train/lora_trainer" target="_blank" rel="noopener noreferrer">SDXL trainer</a>. Training a concept takes around 10 minutes. Once trained, the concept becomes available to use in the creation tool for all of the endpoints, including images and video.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="training">Training<a class="hash-link" href="#training" title="Direct link to heading">​</a></h2><p>You can train a concept through the <a href="https://beta.eden.art/train/lora_trainer" target="_blank" rel="noopener noreferrer">training UI</a> or through the <a href="/docs/guides/sdk">SDK</a>.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="selecting-your-training-set">Selecting your training set<a class="hash-link" href="#selecting-your-training-set" title="Direct link to heading">​</a></h3><p>All you need to train a concept is a few images. The number of images can vary a lot depending on what you are trying to learn. For a face or object, 4-10 images is usually enough, but you can include hundreds or even thousands of images to capture a more diverse style.</p><p>In general, the choice of the training images is the biggest factor determining the quality and accuracy of the concept. With practice you can develop an intuition for how different dataset qualities affect your results. If you&#x27;re unsatisfied with your results, try a different selection of training images before adjusting the settings.</p><p>Some suggestions and tips for choosing training images:</p><ul><li><strong>Selective diversity</strong>: If you&#x27;re trying to learn a specific face or object, your training images should feature the target consistently, and maximize the variance of everything you&#x27;re <em>not</em> trying to learn. For example, if you have several images of a person whose face you are trying to learn, and they are wearing a green shirt in all of the images, the concept is more likely to include the green shirt. It helps to capture the target in different poses, from different angles, and with different lighting conditions or facial expressions. Similarly, if you want to learn an artistic style or aesthetic, you should have a selection of works which is diverse with respect to all the features you&#x27;re <em>not</em> trying to learn (e.g. content, layout, color palettes, etc), while being consistent with respect to the features you are trying to learn (the &quot;style&quot;).</li></ul><p>Some additional guidelines that you should almost always follow:</p><ul><li><strong>High resolution</strong>: Images that are at least 768x768 pixels are best quality. Below that is not recommended as your concept might learn and adopt the low quality of the training images.</li><li><strong>Center-cropped</strong>: The concept trainer will automatically crop your images to the center square. Avoid placing your the target subject outside of that square. </li><li><strong>Prominence</strong>: For faces and objects, aim to feature the target object prominently in the image.</li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="optional-including-your-own-prompts">(Optional) Including your own prompts<a class="hash-link" href="#optional-including-your-own-prompts" title="Direct link to heading">​</a></h3><p>Concepts are trained on a set of images and their corresponding prompts. By default, the concept trainer tries to produce these prompts for you automatically. However, in some cases you may want to prompt your concept in a very specific way. For this, you may optionally upload your own prompts along with the training images, overriding this behavior. </p><p>To do this, create a set of text files whose names match the images (e.g. <code>1.txt</code> for <code>1.jpg</code>, <code>image2.txt</code> for <code>image2.png</code>, etc) and which contain the prompt you want to use for the corresponding image.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="training-parameters">Training parameters<a class="hash-link" href="#training-parameters" title="Direct link to heading">​</a></h3><p>The following parameters are required:</p><ul><li><strong>Training images</strong>: You may upload image files (jpg, png, or webm) directly or upload zip files containing images. You may optionally upload your own prompts for the training images by zipping the images together with a set of text files whose names match the images (see above). You may upload up to 10 files, and each file must be below 100MB.</li><li><strong>Concept name</strong>: This is how you refer to the concept in prompts. You do not need to version since names are not required to be unique.</li><li><strong>Training mode</strong>: There are three available modes: object, face, and style. The mode is used to call upon trainer templates that are optimized for these three categories. Faces refer to human faces, objects are for all other &quot;things&quot; (including non-human faces), and styles refer to abstract style characteristics common to all the training images.</li></ul><p>The following parameters are optional, and rarely need to be changed:</p><ul><li><strong>Train steps</strong>: This refers to how long to finetune the model with your dataset. More steps should lead to fitting your concept more accurately, but too many steps may overfit your concept, leading to poor proptability and generalization, as well as visual artifacts.</li><li><strong>Random flip</strong>: This setting doubles the number of training samples by randomly flipping each image horizontally. This should generally be on unless the subject has a specific horizontal orientation which cannot appear mirrored, for example text or logos. Flipping is always automatically disabled when training in face mode.</li><li><strong>LoRA rank</strong>: The dimension/size of the LoRAs. Higher values allow more &#x27;capacity&#x27; for the model to learn and can be more succesful for complex or diverse objects or styles. But they are more likely to overfit on small image sets and there are diminishing returns.</li><li><strong>Train resolution</strong> : Image resolution used for training. If your training resolution is much lower than the resolution you create with, the concept will appear smaller inside a larger image and will often have repeating artifacts like multiple noses or copies of the same face. Training at lower resolutions (e.g. 768) can be useful if you want to learn a face but want to prompt it in settings where the face is only a small part of the image. Using init_images with rough shape composition can be very helpful in this scenario.</li></ul><div class="theme-admonition theme-admonition-tip alert alert--success admonition_LlT9"><div class="admonitionHeading_tbUL"><span class="admonitionIcon_kALy"><svg viewBox="0 0 12 16"><path fill-rule="evenodd" d="M6.5 0C3.48 0 1 2.19 1 5c0 .92.55 2.25 1 3 1.34 2.25 1.78 2.78 2 4v1h5v-1c.22-1.22.66-1.75 2-4 .45-.75 1-2.08 1-3 0-2.81-2.48-5-5.5-5zm3.64 7.48c-.25.44-.47.8-.67 1.11-.86 1.41-1.25 2.06-1.45 3.23-.02.05-.02.11-.02.17H5c0-.06 0-.13-.02-.17-.2-1.17-.59-1.83-1.45-3.23-.2-.31-.42-.67-.67-1.11C2.44 6.78 2 5.65 2 5c0-2.2 2.02-4 4.5-4 1.22 0 2.36.42 3.22 1.19C10.55 2.94 11 3.94 11 5c0 .66-.44 1.78-.86 2.48zM4 14h5c-.23 1.14-1.3 2-2.5 2s-2.27-.86-2.5-2z"></path></svg></span>tip</div><div class="admonitionContent_S0QG"><p>Training at lower resolutions (e.g. 768) can be useful if you want to learn a face but want to prompt it in settings where the face is only a small part of the image. Using init_images with rough shape composition can be very helpful in this scenario.</p></div></div><p>The trainer is designed to handle most cases well with the default settings. In most cases, you are better off changing the training images than adjusting the optional settings to improve your results. However, some concepts and styles are more challenging to capture well and may require some trial and error adjusting the optional settings to achieve the right balance of diversity, accuracy, and promptability.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="concept-types">Concept types<a class="hash-link" href="#concept-types" title="Direct link to heading">​</a></h2><p>Concepts are a highly versatile and powerful creation tool. They can be used to capture a specific person&#x27;s face or likeness, an animated character, or a complex object. They can also be more abstract, referring to a particular artistic style or genre, or even a compositional pattern without any specific content, such as a <a href="https://www.google.com/search?q=triptych&amp;tbm=isch" target="_blank" rel="noopener noreferrer">triptych</a> or a <a href="https://www.google.com/search?q=knolling&amp;tbm=isch" target="_blank" rel="noopener noreferrer">knolling</a>.</p><p>While concepts can be trained on any arbitrary image set, in practice there are three main categories of concepts: faces, objects, and styles. Internally, each of these categories has a different training mode which is optimized for that category.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="faces">Faces<a class="hash-link" href="#faces" title="Direct link to heading">​</a></h3><p>Generative models like Stable Diffusion are great at generating realistic human faces. However, the model obviously doesn&#x27;t know what every non-famous person looks like. To get around this limitation, we can train a concept to learn a specific person&#x27;s face.</p><div class="theme-admonition theme-admonition-warning alert alert--danger admonition_LlT9"><div class="admonitionHeading_tbUL"><span class="admonitionIcon_kALy"><svg viewBox="0 0 12 16"><path fill-rule="evenodd" d="M5.05.31c.81 2.17.41 3.38-.52 4.31C3.55 5.67 1.98 6.45.9 7.98c-1.45 2.05-1.7 6.53 3.53 7.7-2.2-1.16-2.67-4.52-.3-6.61-.61 2.03.53 3.33 1.94 2.86 1.39-.47 2.3.53 2.27 1.67-.02.78-.31 1.44-1.13 1.81 3.42-.59 4.78-3.42 4.78-5.56 0-2.84-2.53-3.22-1.25-5.61-1.52.13-2.03 1.13-1.89 2.75.09 1.08-1.02 1.8-1.86 1.33-.67-.41-.66-1.19-.06-1.78C8.18 5.31 8.68 2.45 5.05.32L5.03.3l.02.01z"></path></svg></span>danger</div><div class="admonitionContent_S0QG"><p>Note that the faces mode is highly optimized for human faces. If you want to train a concept to learn a non-human face, such as a cartoon character or animal, you will probably get better results using the <a href="#objects">object</a> mode instead.</p></div></div><div class="theme-admonition theme-admonition-tip alert alert--success admonition_LlT9"><div class="admonitionHeading_tbUL"><span class="admonitionIcon_kALy"><svg viewBox="0 0 12 16"><path fill-rule="evenodd" d="M6.5 0C3.48 0 1 2.19 1 5c0 .92.55 2.25 1 3 1.34 2.25 1.78 2.78 2 4v1h5v-1c.22-1.22.66-1.75 2-4 .45-.75 1-2.08 1-3 0-2.81-2.48-5-5.5-5zm3.64 7.48c-.25.44-.47.8-.67 1.11-.86 1.41-1.25 2.06-1.45 3.23-.02.05-.02.11-.02.17H5c0-.06 0-.13-.02-.17-.2-1.17-.59-1.83-1.45-3.23-.2-.31-.42-.67-.67-1.11C2.44 6.78 2 5.65 2 5c0-2.2 2.02-4 4.5-4 1.22 0 2.36.42 3.22 1.19C10.55 2.94 11 3.94 11 5c0 .66-.44 1.78-.86 2.48zM4 14h5c-.23 1.14-1.3 2-2.5 2s-2.27-.86-2.5-2z"></path></svg></span>tip</div><div class="admonitionContent_S0QG"><p>When uploading faces, it&#x27;s usually a good idea to crop the images so the face fills a large fraction of the total image.</p></div></div><p align="center"><img src="https://storage.googleapis.com/public-assets-xander/A_workbox/eden_docs/xander_training_images.jpg" alt="Face training images of Xander"><br><small style="color:gray">Face training images of Xander</small></p><p>After training a concept on these images (which we will name &quot;Xander&quot;), we can generate creations with it by selecting the concept in the creation interface. To refer to the concept in your prompt, you can include the concept name or <code>&lt;concept&gt;</code> in your prompt, this process is not case-sensitive. For example, any of the below will work:</p><ul><li>Xander as a character in a noir graphic novel</li><li>a xander action figure</li><li>&lt;concept<!-- -->&gt;<!-- --> as a knight in shining armour</li><li>&lt;Xander<!-- -->&gt;<!-- --> as the Mona Lisa</li></ul><p align="center"><img src="https://storage.googleapis.com/public-assets-xander/A_workbox/eden_docs/xander_generated_images.jpg" alt="Generated images with the Xander concept."><br><small style="color:gray">Generated images with the Xander concept.</small></p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="objects">Objects<a class="hash-link" href="#objects" title="Direct link to heading">​</a></h3><p>The &quot;Object&quot; training mode is optimized for learning all other &quot;things&quot; besides for human faces. This includes non-human faces, physical objects, characters, cartoons, and miscellaneous objects.</p><p>For example, the images below are professional renders of the character <a href="https://twitter.com/kojii_ai" target="_blank" rel="noopener noreferrer">Kojii</a>. They exemplify a good training set: a single, consistent character with subtle variations in pose and appearance between every image. </p><div class="theme-admonition theme-admonition-tip alert alert--success admonition_LlT9"><div class="admonitionHeading_tbUL"><span class="admonitionIcon_kALy"><svg viewBox="0 0 12 16"><path fill-rule="evenodd" d="M6.5 0C3.48 0 1 2.19 1 5c0 .92.55 2.25 1 3 1.34 2.25 1.78 2.78 2 4v1h5v-1c.22-1.22.66-1.75 2-4 .45-.75 1-2.08 1-3 0-2.81-2.48-5-5.5-5zm3.64 7.48c-.25.44-.47.8-.67 1.11-.86 1.41-1.25 2.06-1.45 3.23-.02.05-.02.11-.02.17H5c0-.06 0-.13-.02-.17-.2-1.17-.59-1.83-1.45-3.23-.2-.31-.42-.67-.67-1.11C2.44 6.78 2 5.65 2 5c0-2.2 2.02-4 4.5-4 1.22 0 2.36.42 3.22 1.19C10.55 2.94 11 3.94 11 5c0 .66-.44 1.78-.86 2.48zM4 14h5c-.23 1.14-1.3 2-2.5 2s-2.27-.86-2.5-2z"></path></svg></span>tip</div><div class="admonitionContent_S0QG"><p>We&#x27;re used to &quot;more data is always better&quot;, but for concept training, 10 really good, diverse HD images is usually better than 100 low-quality or similar images.</p></div></div><p align="center"><img src="https://storage.googleapis.com/public-assets-xander/A_workbox/eden_docs/koji_training_imgs.jpg" alt="Kojii character training images."><br><small style="color:gray">Kojii character training images.</small></p><p>After training the concept, we are again able to compose with it in the creation tool. Eg, we can prompt:</p><ul><li>a photo of &lt;kojii<!-- -->&gt;<!-- --> surfing a wave</li><li>kojii in a snowglobe</li><li>a photo of &lt;concept<!-- -->&gt;<!-- --> climbing mount Everest</li><li>a low-poly artwork of Kojii</li></ul><p align="center"><img src="https://storage.googleapis.com/public-assets-xander/A_workbox/eden_docs/koji_grid.jpg" alt="Generated images with the Kojii concept."><br><small style="color:gray">Generated images with the Kojii concept.</small></p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="styles">Styles<a class="hash-link" href="#styles" title="Direct link to heading">​</a></h3><p>Concepts can also be used to model artistic styles or genres. The way that style concepts differ from object concepts is that style concepts are not trained on specific content or &quot;things&quot;, but rather on the abstract style characteristics common to all the training images. </p><p>For example, the images below are artworks originally created by <a href="https://vjsuave.com/" target="_blank" rel="noopener noreferrer">VJ Suave</a>.</p><p align="center"><img src="https://storage.googleapis.com/public-assets-xander/A_workbox/eden_docs/suave_training_imgs.jpg" alt="Training images to learn the VJ Suave visual style."><br><small style="color:gray">Training images to learn the VJ Suave visual style.</small></p><p>Like objects and faces, you can compose with the style concept in the creation tool by referring to it in your prompt. Particularly with style concepts, you don&#x27;t have to refer to the concept at all, and can simply prompt as you normally would, and the active concept will be used to influence the image.</p><p>The following are samples are all generated from the trained Suave concept.</p><p align="center"><img src="https://storage.googleapis.com/public-assets-xander/A_workbox/eden_docs/suave_generated_imgs.jpg" alt="Generated images with the Suave style concept."><br><small style="color:gray">Generated images with the Suave style concept.</small></p><p>Styles are the most abstract of all the training modes, and can be used to capture a wide variety of aesthetics and compositional patterns. The most common use case for style concepts is to capture a particular artist&#x27;s style or a genre such as cubism or vaporwave. However, style concepts can be used to capture non-aesthetic visual motifs, such as color palettes, layout patterns, or even more abstract notions.</p><p>For example, a <a href="https://www.google.com/search?q=knolling&amp;tbm=isch" target="_blank" rel="noopener noreferrer">knolling</a> is a photograph where related objects are arranged neatly in a grid-like pattern. The images below are a training set of three knolling images.</p><p align="center"><img src="/img/knolling_training.jpg" alt="Knolling training set."><br><small style="color:gray">Knolling training set.</small></p><p>Note that these images are not of the same objects, nor do they share a common visual aesthetic with each other. Their only connection is the novel layout. Despite that, training a style concept on these images results in a concept that learns the knolling layout pattern.</p><p align="center"><img src="/img/knolling_generated.jpg" alt="Images generated using the Knolling concept."><br><small style="color:gray">Images generated using the Knolling concept.</small></p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="generating-with-concepts">Generating with concepts<a class="hash-link" href="#generating-with-concepts" title="Direct link to heading">​</a></h2><p>Once a concept is trained, you may select your concept in the <a href="https://beta.eden.art/create" target="_blank" rel="noopener noreferrer">creation tool</a> from the concept selector, and trigger the concept by referring to it in the prompt by its name or by &lt;concept<!-- -->&gt;<!-- -->.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="exporting-loras-for-use-in-other-tools">Exporting LoRas for use in other tools<a class="hash-link" href="#exporting-loras-for-use-in-other-tools" title="Direct link to heading">​</a></h2><p>Eden concepts are trained using the LoRA technique, a widely used extension to Stable Diffusion, and is fully compatible with the many other tools that support it. You may export your concepts as LoRas to use in other tools, such as <a href="https://github.com/AUTOMATIC1111/stable-diffusion-webui" target="_blank" rel="noopener noreferrer">AUTOMATIC1111</a> or <a href="https://github.com/comfyanonymous/ComfyUI" target="_blank" rel="noopener noreferrer">ComfyUI</a>.</p><p>To export your concept, you can download it from the app. The concept comes as a .tar file which contains two files, one with the token embeddings and one with the LoRA weights.</p><p align="center"><img src="/img/download_concept.jpg" alt="Download your concept LoRa as a .tar file"><br><small style="color:gray">Download your concept LoRa as a .tar file</small></p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="automatic1111-stable-diffusion-webui">AUTOMATIC1111 stable-diffusion-webui<a class="hash-link" href="#automatic1111-stable-diffusion-webui" title="Direct link to heading">​</a></h3><p>To use your concept in <a href="https://github.com/AUTOMATIC1111/stable-diffusion-webui" target="_blank" rel="noopener noreferrer">AUTOMATIC1111</a>, follow these steps:</p><ol><li>Download the concept.</li><li>Extract (untar) the content.</li><li>Put the <code>[lora_name]_lora.safetensors</code> file in the <code>stable-diffusion-webui/models/Lora</code> folder.</li><li>Put the <code>[lora_name]_embeddings.safetensors</code> file in the <code>stable-diffusion-webui/embeddings</code> folder.</li><li>Eden LoRAs are currently trained using the <a href="https://civitai.com/models/133005/juggernaut-xl" target="_blank" rel="noopener noreferrer"><strong>JuggernautXL_v6</strong> checkpoint</a>. For best results, use that same model as your base checkpoint.</li><li><strong>Make sure to load both the embedding <em>and</em> the lora weights by triggering them in your prompt</strong></li></ol><p align="center"><img src="/img/auto1111.jpg" alt="Using an exported LoRA in AUTOMATIC1111. Notice how both the token embedding and the LoRa are triggered in the prompt."><br><small style="color:gray">Using an exported LoRA in AUTOMATIC1111. Notice how both the token embedding and the LoRa are triggered in the prompt.</small></p><div class="theme-admonition theme-admonition-tip alert alert--success admonition_LlT9"><div class="admonitionHeading_tbUL"><span class="admonitionIcon_kALy"><svg viewBox="0 0 12 16"><path fill-rule="evenodd" d="M6.5 0C3.48 0 1 2.19 1 5c0 .92.55 2.25 1 3 1.34 2.25 1.78 2.78 2 4v1h5v-1c.22-1.22.66-1.75 2-4 .45-.75 1-2.08 1-3 0-2.81-2.48-5-5.5-5zm3.64 7.48c-.25.44-.47.8-.67 1.11-.86 1.41-1.25 2.06-1.45 3.23-.02.05-.02.11-.02.17H5c0-.06 0-.13-.02-.17-.2-1.17-.59-1.83-1.45-3.23-.2-.31-.42-.67-.67-1.11C2.44 6.78 2 5.65 2 5c0-2.2 2.02-4 4.5-4 1.22 0 2.36.42 3.22 1.19C10.55 2.94 11 3.94 11 5c0 .66-.44 1.78-.86 2.48zM4 14h5c-.23 1.14-1.3 2-2.5 2s-2.27-.86-2.5-2z"></path></svg></span>tip</div><div class="admonitionContent_S0QG"><p>For &quot;face&quot; and &quot;object&quot; modes, refer to your concept directly by using <em>[lora_name]<!-- -->_embeddings</em> in the prompt. For style concepts, you should include <em>&quot;... in the style of <!-- -->[lora_name]<!-- -->_embeddings&quot;</em> in your prompt.</p></div></div><h3 class="anchor anchorWithStickyNavbar_LWe7" id="comfyui">ComfyUI<a class="hash-link" href="#comfyui" title="Direct link to heading">​</a></h3><ol><li>Download the concept.</li><li>Extract (untar) the content.</li><li>Put the <code>[lora_name]_lora.safetensors</code> file in the <code>ComfyUI/models/loras</code> folder.</li><li>Put the <code>[lora_name]_embeddings.safetensors</code> file in the <code>ComfyUI/models/embeddings</code> folder.</li><li>For best results, use that same model as your base checkpoint used for training the LoRa.</li><li>Load the LoRA weights using a <em>&quot;Load LoRA&quot;</em> node in your pipeline and adjust the strength as needed.</li><li>Trigger the concept in your prompt by refering to it with <em>embedding:<!-- -->[lora_name]<!-- -->_embeddings</em>. The strength of the token can also be modulated, but its recommended to keep it high.</li></ol><p align="center"><img src="/img/comfy.jpeg" alt="Using an Eden concept in ComfyUI, make sure to trigger the embeddings in the prompt!"><br><small style="color:gray">Using an Eden concept in ComfyUI, make sure to trigger the embeddings in the prompt!</small></p><div class="theme-admonition theme-admonition-tip alert alert--success admonition_LlT9"><div class="admonitionHeading_tbUL"><span class="admonitionIcon_kALy"><svg viewBox="0 0 12 16"><path fill-rule="evenodd" d="M6.5 0C3.48 0 1 2.19 1 5c0 .92.55 2.25 1 3 1.34 2.25 1.78 2.78 2 4v1h5v-1c.22-1.22.66-1.75 2-4 .45-.75 1-2.08 1-3 0-2.81-2.48-5-5.5-5zm3.64 7.48c-.25.44-.47.8-.67 1.11-.86 1.41-1.25 2.06-1.45 3.23-.02.05-.02.11-.02.17H5c0-.06 0-.13-.02-.17-.2-1.17-.59-1.83-1.45-3.23-.2-.31-.42-.67-.67-1.11C2.44 6.78 2 5.65 2 5c0-2.2 2.02-4 4.5-4 1.22 0 2.36.42 3.22 1.19C10.55 2.94 11 3.94 11 5c0 .66-.44 1.78-.86 2.48zM4 14h5c-.23 1.14-1.3 2-2.5 2s-2.27-.86-2.5-2z"></path></svg></span>tip</div><div class="admonitionContent_S0QG"><p>For &quot;face&quot; and &quot;object&quot; modes you refer to your concept directly by using <em>embedding:<!-- -->[lora_name]<!-- -->_embeddings</em> somewhere in the prompt, for style concepts you should add <em>&quot;... in the style of embedding:<!-- -->[lora_name]<!-- -->_embeddings&quot;</em> somewhere in your prompt.</p></div></div><div class="theme-admonition theme-admonition-note alert alert--secondary admonition_LlT9"><div class="admonitionHeading_tbUL"><span class="admonitionIcon_kALy"><svg viewBox="0 0 14 16"><path fill-rule="evenodd" d="M6.3 5.69a.942.942 0 0 1-.28-.7c0-.28.09-.52.28-.7.19-.18.42-.28.7-.28.28 0 .52.09.7.28.18.19.28.42.28.7 0 .28-.09.52-.28.7a1 1 0 0 1-.7.3c-.28 0-.52-.11-.7-.3zM8 7.99c-.02-.25-.11-.48-.31-.69-.2-.19-.42-.3-.69-.31H6c-.27.02-.48.13-.69.31-.2.2-.3.44-.31.69h1v3c.02.27.11.5.31.69.2.2.42.31.69.31h1c.27 0 .48-.11.69-.31.2-.19.3-.42.31-.69H8V7.98v.01zM7 2.3c-3.14 0-5.7 2.54-5.7 5.68 0 3.14 2.56 5.7 5.7 5.7s5.7-2.55 5.7-5.7c0-3.15-2.56-5.69-5.7-5.69v.01zM7 .98c3.86 0 7 3.14 7 7s-3.14 7-7 7-7-3.12-7-7 3.14-7 7-7z"></path></svg></span>note</div><div class="admonitionContent_S0QG"><p>You may notice that the LoRA strength has a relatively small effect on the final output. This is because Eden concepts optimize towards using the token embedding to learn most of the concept, rather than the LoRA matrices.</p></div></div></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="theme-doc-footer-edit-meta-row row"><div class="col"></div><div class="col lastUpdated_vwxv"><span class="theme-last-updated">Last updated<!-- --> on <b><time datetime="2025-01-31T16:42:57.000Z">Jan 31, 2025</time></b></span></div></div></footer></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="Docs pages navigation"><a class="pagination-nav__link pagination-nav__link--prev" href="/docs/category/guides"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">Guides</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/docs/guides/agents"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">Creative AI Agents (w FLUX LoRA)</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#training" class="table-of-contents__link toc-highlight">Training</a><ul><li><a href="#selecting-your-training-set" class="table-of-contents__link toc-highlight">Selecting your training set</a></li><li><a href="#optional-including-your-own-prompts" class="table-of-contents__link toc-highlight">(Optional) Including your own prompts</a></li><li><a href="#training-parameters" class="table-of-contents__link toc-highlight">Training parameters</a></li></ul></li><li><a href="#concept-types" class="table-of-contents__link toc-highlight">Concept types</a><ul><li><a href="#faces" class="table-of-contents__link toc-highlight">Faces</a></li><li><a href="#objects" class="table-of-contents__link toc-highlight">Objects</a></li><li><a href="#styles" class="table-of-contents__link toc-highlight">Styles</a></li></ul></li><li><a href="#generating-with-concepts" class="table-of-contents__link toc-highlight">Generating with concepts</a></li><li><a href="#exporting-loras-for-use-in-other-tools" class="table-of-contents__link toc-highlight">Exporting LoRas for use in other tools</a><ul><li><a href="#automatic1111-stable-diffusion-webui" class="table-of-contents__link toc-highlight">AUTOMATIC1111 stable-diffusion-webui</a></li><li><a href="#comfyui" class="table-of-contents__link toc-highlight">ComfyUI</a></li></ul></li></ul></div></div></div></div></main></div></div><footer class="footer footer--dark"><div class="container container-fluid"><div class="row footer__links"><div class="col footer__col"><div class="footer__title">Links</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://app.eden.art" target="_blank" rel="noopener noreferrer" class="footer__link-item">App<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a class="footer__link-item" href="/docs/category/overview">Docs</a></li></ul></div><div class="col footer__col"><div class="footer__title">Community</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://discord.gg/4dSYwDT" target="_blank" rel="noopener noreferrer" class="footer__link-item">Discord<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://twitter.com/eden_art_" target="_blank" rel="noopener noreferrer" class="footer__link-item">Twitter<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://instagram.com/eden.art____" target="_blank" rel="noopener noreferrer" class="footer__link-item">Instagram<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div><div class="col footer__col"><div class="footer__title">Developers</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://github.com/github.com/edenartlab" target="_blank" rel="noopener noreferrer" class="footer__link-item">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://www.npmjs.com/package/@edenlabs/eden-sdk" target="_blank" rel="noopener noreferrer" class="footer__link-item">Eden SDK (JS)<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://github.com/abraham-ai/eden-sdk-py" target="_blank" rel="noopener noreferrer" class="footer__link-item">Eden SDK (Python)<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright"> </div></div></div></footer></div>
<script src="/assets/js/runtime~main.3ed09d88.js"></script>
<script src="/assets/js/main.e16c5f17.js"></script>
</body>
</html>